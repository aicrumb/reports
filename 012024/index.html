<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>

<style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color:#ffffff; --text-color:#333333; --select-text-bg-color:#B5D6FC; --select-text-font-color:auto; --monospace:"Lucida Console",Consolas,"Courier",monospace; --title-bar-height:20px; }
.mac-os-11 { --title-bar-height:28px; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
h1, h2, h3, h4, h5 { white-space: pre-wrap; }
body { margin: 0px; padding: 0px; height: auto; inset: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; overflow-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 36px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
.typora-export .task-list-item input { pointer-events: none; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; image-orientation: from-image; }
button, input, select, textarea { color: inherit; font: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 4; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px; }
thead, tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror-linenumber { user-select: none; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-fences-adv-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
svg { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li blockquote { margin: 1rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; font-variant-ligatures: no-common-ligatures; }
  #write { margin-top: 0px; border-color: transparent !important; padding-top: 0px !important; padding-bottom: 0px !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  .typora-export #write { break-after: avoid; }
  .typora-export #write::after { height: 0px; }
  .is-mac table { break-inside: avoid; }
  #write > p:nth-child(1) { margin-top: 0px; }
  .typora-export-show-outline .typora-export-sidebar { display: none; }
  figure { overflow-x: visible; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
#write.first-line-indent p > .md-image:only-child:not(.md-img-error) img { left: -2em; position: relative; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
.MathJax_ref { fill: currentcolor; }
[contenteditable="true"]:active, [contenteditable="true"]:focus, [contenteditable="false"]:active, [contenteditable="false"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.reversefootnote { font-family: ui-monospace, sans-serif; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.6; font-family: var(--monospace); }
code { text-align: left; vertical-align: initial; }
a.md-print-anchor { white-space: pre !important; border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="flow"] svg, [lang="mermaid"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }
mark { background: rgb(255, 255, 0); color: rgb(0, 0, 0); }
.md-html-inline .md-plain, .md-html-inline strong, mark .md-inline-math, mark strong { color: inherit; }
.md-expand mark .md-meta { opacity: 0.3 !important; }
mark .md-meta { color: rgb(0, 0, 0); }
@media print {
  .typora-export h1, .typora-export h2, .typora-export h3, .typora-export h4, .typora-export h5, .typora-export h6 { break-inside: avoid; }
}
.md-diagram-panel .messageText { stroke: none !important; }
.md-diagram-panel .start-state { fill: var(--node-fill); }
.md-diagram-panel .edgeLabel rect { opacity: 1 !important; }
.md-fences.md-fences-math { font-size: 1em; }
.md-fences-advanced:not(.md-focus) { padding: 0px; white-space: nowrap; border: 0px; }
.md-fences-advanced:not(.md-focus) { background: inherit; }
.typora-export-show-outline .typora-export-content { max-width: 1440px; margin: auto; display: flex; flex-direction: row; }
.typora-export-sidebar { width: 300px; font-size: 0.8rem; margin-top: 80px; margin-right: 18px; }
.typora-export-show-outline #write { --webkit-flex:2; flex: 2 1 0%; }
.typora-export-sidebar .outline-content { position: fixed; top: 0px; max-height: 100%; overflow: hidden auto; padding-bottom: 30px; padding-top: 60px; width: 300px; }
@media screen and (max-width: 1024px) {
  .typora-export-sidebar, .typora-export-sidebar .outline-content { width: 240px; }
}
@media screen and (max-width: 800px) {
  .typora-export-sidebar { display: none; }
}
.outline-content li, .outline-content ul { margin-left: 0px; margin-right: 0px; padding-left: 0px; padding-right: 0px; list-style: none; overflow-wrap: anywhere; }
.outline-content ul { margin-top: 0px; margin-bottom: 0px; }
.outline-content strong { font-weight: 400; }
.outline-expander { width: 1rem; height: 1.42857rem; position: relative; display: table-cell; vertical-align: middle; cursor: pointer; padding-left: 4px; }
.outline-expander::before { content: ""; position: relative; font-family: Ionicons; display: inline-block; font-size: 8px; vertical-align: middle; }
.outline-item { padding-top: 3px; padding-bottom: 3px; cursor: pointer; }
.outline-expander:hover::before { content: ""; }
.outline-h1 > .outline-item { padding-left: 0px; }
.outline-h2 > .outline-item { padding-left: 1em; }
.outline-h3 > .outline-item { padding-left: 2em; }
.outline-h4 > .outline-item { padding-left: 3em; }
.outline-h5 > .outline-item { padding-left: 4em; }
.outline-h6 > .outline-item { padding-left: 5em; }
.outline-label { cursor: pointer; display: table-cell; vertical-align: middle; text-decoration: none; color: inherit; }
.outline-label:hover { text-decoration: underline; }
.outline-item:hover { border-color: rgb(245, 245, 245); background-color: var(--item-hover-bg-color); }
.outline-item:hover { margin-left: -28px; margin-right: -28px; border-left: 28px solid transparent; border-right: 28px solid transparent; }
.outline-item-single .outline-expander::before, .outline-item-single .outline-expander:hover::before { display: none; }
.outline-item-open > .outline-item > .outline-expander::before { content: ""; }
.outline-children { display: none; }
.info-panel-tab-wrapper { display: none; }
.outline-item-open > .outline-children { display: block; }
.typora-export .outline-item { padding-top: 1px; padding-bottom: 1px; }
.typora-export .outline-item:hover { margin-right: -8px; border-right: 8px solid transparent; }
.typora-export .outline-expander::before { content: "+"; font-family: inherit; top: -1px; }
.typora-export .outline-expander:hover::before, .typora-export .outline-item-open > .outline-item > .outline-expander::before { content: "−"; }
.typora-export-collapse-outline .outline-children { display: none; }
.typora-export-collapse-outline .outline-item-open > .outline-children, .typora-export-no-collapse-outline .outline-children { display: block; }
.typora-export-no-collapse-outline .outline-expander::before { content: "" !important; }
.typora-export-show-outline .outline-item-active > .outline-item .outline-label { font-weight: 700; }
.md-inline-math-container mjx-container { zoom: 0.95; }
mjx-container { break-inside: avoid; }


html {
	font-size: 19px;
}

html, body {
	margin: auto;
	background: #fefefe;
	-webkit-font-smoothing: antialiased;
}
body {
	font-family: "Vollkorn", Palatino, Times;
	color: #333;
	line-height: 1.4;
	text-align: justify;
}

#write {
	max-width: 960px;
	margin: 0 auto;
	margin-bottom: 2em;
	line-height: 1.53;
	padding-top: 40px;
}

@media only screen and (min-width: 1400px) {
	#write {
		max-width: 1100px;
	}
}

@media print {
	html {
		font-size: 13px;
	}
}

/* Typography
-------------------------------------------------------- */

#write>h1:first-child,
h1 {
	margin-top: 1.6em;
	font-weight: normal;
}

h1 {
	font-size:3em;
}

h2 {
	margin-top:2em;
	font-weight: normal;
}

h3 {
	font-weight: normal;
	font-style: italic;
	margin-top: 3em;
}

h1, 
h2, 
h3{
	text-align: center;
}

h2:after{
	border-bottom: 1px solid #2f2f2f;
    content: '';
    width: 100px;
    display: block;
    margin: 0 auto;
    height: 1px;
}

h1+h2, h2+h3 {
	margin-top: 0.83em;
}

p,
.mathjax-block {
	margin-top: 0;
	-webkit-hypens: auto;
	-moz-hypens: auto;
	hyphens: auto;
}
ul {
	list-style: square;
	padding-left: 1.2em;
}
ol {
	padding-left: 1.2em;
}

@media print {
	ol {
		padding-left: 40px;
	}
}

blockquote {
	margin-left: 1em;
	padding-left: 1em;
	border-left: 1px solid #ddd;
}
code,
pre {
	font-family: "Consolas", "Menlo", "Monaco", monospace, serif;
	font-size: .9em;
	background: white;
}
.md-fences{
	margin-left: 1em;
	padding-left: 1em;
	border: 1px solid #ddd;
	padding-bottom: 8px;
	padding-top: 6px;
	margin-bottom: 1.5em;
}

a {
	color: #2484c1;
	text-decoration: none;
}
a:hover {
	text-decoration: underline;
}
a img {
	border: none;
}
h1 a,
h1 a:hover {
	color: #333;
	text-decoration: none;
}
hr {
	color: #ddd;
	height: 1px;
	margin: 2em 0;
	border-top: solid 1px #ddd;
	border-bottom: none;
	border-left: 0;
	border-right: 0;
}
.ty-table-edit {
	background: #ededed;
    padding-top: 4px;
}
table {
	margin-bottom: 1.333333rem
}
table th,
table td {
	padding: 8px;
	line-height: 1.333333rem;
	vertical-align: top;
	border-top: 1px solid #ddd
}
table th {
	font-weight: bold
}
table thead th {
	vertical-align: bottom
}
table caption+thead tr:first-child th,
table caption+thead tr:first-child td,
table colgroup+thead tr:first-child th,
table colgroup+thead tr:first-child td,
table thead:first-child tr:first-child th,
table thead:first-child tr:first-child td {
	border-top: 0
}
table tbody+tbody {
	border-top: 2px solid #ddd
}

.task-list{
	padding:0;
}

.md-task-list-item {
	padding-left: 1.6rem;
}

.md-task-list-item > input:before {
	content: '\221A';
	display: inline-block;
	width: 1.33333333rem;
  	height: 1.6rem;
	vertical-align: middle;
	text-align: center;
	color: #ddd;
	background-color: #fefefe;
}

.md-task-list-item > input:checked:before,
.md-task-list-item > input[checked]:before{
	color: inherit;
}
.md-tag {
	color: inherit;
	font: inherit;
}
#write pre.md-meta-block {
	min-height: 35px;
	padding: 0.5em 1em;
}
#write pre.md-meta-block {
	white-space: pre;
	background: #f8f8f8;
	border: 0px;
	color: #999;
	
	width: 100vw;
	max-width: calc(100% + 60px);
	margin-left: -30px;
	border-left: 30px #f8f8f8 solid;
	border-right: 30px #f8f8f8 solid;

	margin-bottom: 2em;
	margin-top: -1.3333333333333rem;
	padding-top: 26px;
	padding-bottom: 10px;
	line-height: 1.8em;
	font-size: 0.9em;
	font-size: 0.76em;
	padding-left: 0;
}
.md-img-error.md-image>.md-meta{
	vertical-align: bottom;
}
#write>h5.md-focus:before {
	top: 2px;
}

.md-toc {
	margin-top: 40px;
}

.md-toc-content {
	padding-bottom: 20px;
}

.outline-expander:before {
	color: inherit;
	font-size: 14px;
	top: auto;
	content: "\f0da";
	font-family: FontAwesome;
}

.outline-expander:hover:before,
.outline-item-open>.outline-item>.outline-expander:before {
  	content: "\f0d7";
}

/** source code mode */
#typora-source {
	font-family: Courier, monospace;
    color: #6A6A6A;
}

.html-for-mac #typora-sidebar {
    -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, .175);
    box-shadow: 0 6px 12px rgba(0, 0, 0, .175);
}

.cm-s-typora-default .cm-header, 
.cm-s-typora-default .cm-property,
.CodeMirror.cm-s-typora-default div.CodeMirror-cursor {
	color: #428bca;
}

.cm-s-typora-default .cm-atom, .cm-s-typora-default .cm-number {
	color: #777777;
}

.typora-node .file-list-item-parent-loc, 
.typora-node .file-list-item-time, 
.typora-node .file-list-item-summary {
	font-family: arial, sans-serif;
}

.md-task-list-item>input {
    margin-left: -1.3em;
    margin-top: calc(1rem - 12px);
}

.md-mathjax-midline {
	background: #fafafa;
}

.md-fences .code-tooltip {
	bottom: -2em !important;
}

.dropdown-menu .divider {
	border-color: #e5e5e5;
}


</style><style>
        canvas {
            position: fixed; /* Fixed position to cover the whole screen */
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: 1; /* Behind the content */
        }

        /* Your content styling */
        .content {
            position: relative;
            z-index: 2; /* Above the canvas */
            /* Additional styling here */
        }
  </style><title>final report 2</title>
</head>
<body class='typora-export os-windows typora-export-show-outline typora-export-no-collapse-outline'><div class='content' class='typora-export-content'>
<div class="typora-export-sidebar"><div class="outline-content"><li class="outline-item-wrapper outline-h1"><div class="outline-item"><span class="outline-expander"></span><a class="outline-label" href="#january">January</a></div><ul class="outline-children"><li class="outline-item-wrapper outline-h2"><div class="outline-item"><span class="outline-expander"></span><a class="outline-label" href="#llama2-70b-embeddings">Llama2-70b Embeddings</a></div><ul class="outline-children"><li class="outline-item-wrapper outline-h4"><div class="outline-item"><span class="outline-expander"></span><a class="outline-label" href="#performance">Performance</a></div><ul class="outline-children"></ul></li></ul></li><li class="outline-item-wrapper outline-h2"><div class="outline-item"><span class="outline-expander"></span><a class="outline-label" href="#some-more-things-that-didnt-work-very-well">Some (More) Things that Didn't Work Very Well</a></div><ul class="outline-children"><li class="outline-item-wrapper outline-h4"><div class="outline-item"><span class="outline-expander"></span><a class="outline-label" href="#virtual-token-models">Virtual Token Models</a></div><ul class="outline-children"></ul></li><li class="outline-item-wrapper outline-h4"><div class="outline-item"><span class="outline-expander"></span><a class="outline-label" href="#mixture-of-embeddings">Mixture of Embeddings</a></div><ul class="outline-children"></ul></li></ul></li><li class="outline-item-wrapper outline-h2"><div class="outline-item"><span class="outline-expander"></span><a class="outline-label" href="#fin">Fin</a></div><ul class="outline-children"><li class="outline-item-wrapper outline-h4"><div class="outline-item"><span class="outline-expander"></span><a class="outline-label" href="#final-extra-notes">Final (Extra) Notes</a></div><ul class="outline-children"></ul></li></ul></li></ul></li></div></div><div id='write'  class=''><h1 id='january'><span>January</span></h1><hr /><p><span>I am committing myself to writing about what I&#39;ve done every month this year. This month I didn&#39;t have an experiment in mind and didn&#39;t have the idea to write every month until nearly halfway through it. I&#39;m going to speed through some of the first things I did this month. I started with the GPT-2 architecture and gradually modified the code to resemble the Llama architecture. Throughout that process I tried scaled sinusoidal embeddings, cartesian-to-hyperspherical conversions inside the embedding layer, a LayerNorm after the embedding layer, and  more that didn&#39;t turn out well. I decided then to try &quot;parallel decoder blocks&quot; (</span><code>ffn(x) + attn(x) + x</code><span> instead of </span><code>ffn(attn(x)) + x</code><span>) and I got a notably lower loss, so kept that modified architecture throughout the rest of the months&#39; work and called it ParaLlama </span><strong><span>(this probably isn&#39;t normal, I ran another control with the llama-like blocks and it got a better loss than the parallel models with all the same parameters, I should have run a few at the beginning at different seeds)</span></strong><span>.</span></p><h2 id='llama2-70b-embeddings'><span>Llama2-70b Embeddings</span></h2><p><span>I then started exploring using pretrained token embeddings from Llama2-70b in these models, by just simply adding a linear projection from a hidden size of 8192 to a smaller dimension after the token embedding, and from that smaller dimension to 8192 before the language modelling head. Using a linear projection also allows us to &quot;flatten&quot; the parameters down to the size of a model trained without the large embeddings, by simply performing a matrix multiplication between the projection and layer from Llama2-70b, which is super useful for inference! Early on, tried using different percentages of training allocated to &quot;flattened mode&quot; to see if it gave us any gains in loss but the best value ended up being 0% by a landslide. I also reran the small model with a learnable alpha for each residual operation set to 0 at initialization to tend the model&#39;s function closer to identity (ParaLlama-αp) which is something that &quot;seems like it&#39;d be done in those papers with the parallel blocks, idk, right? I think they do that.&quot; It didn&#39;t show any improvements here though, so it&#39;s left out in later (parallel) models.</span></p><p><em><span>EDIT: The learnable 0 alpha for residual ops looks like it could have come from </span><a href='[2003.04887.pdf (arxiv.org)](https://arxiv.org/pdf/2003.04887.pdf)'><span>ReZero</span></a><span>!</span></em></p><figure class='table-figure'><table><thead><tr><th><span>Architecture/Nickname</span></th><th><span>Dim</span></th><th><span>Max Parameters</span></th><th><span>Non-embed Parameters</span></th><th><span>Approximate duration</span></th><th><span>FLOPs</span></th><th><span>Tokens</span></th><th><span>Test loss</span></th><th><span>Average</span></th><th><span>ARC@25</span></th><th><span>TruthfulQA@0</span></th><th><span>WinoGrande@5</span></th><th><span>HellaSwag@10</span></th><th><span>MMLU@5</span></th></tr></thead><tbody><tr><td><a href='https://hf.co/crumb/ParaLlama-p-medium'><span>ParaLlama-p-medium</span></a></td><td><span>1088</span></td><td><span>545m</span></td><td>&nbsp;</td><td><span>38h</span></td><td><span>8.660x10^18</span></td><td><span>5.1x10^9</span></td><td><span>2.295</span></td><td><span>35.018</span></td><td><span>22.01</span></td><td><span>45.43</span></td><td><span>51.54</span></td><td><span>30.09</span></td><td><span>26.02</span></td></tr><tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr><tr><td><a href='https://hf.co/crumb/ParaLlama-p-small'><span>ParaLlama-p-small</span></a></td><td><span>768</span></td><td><span>369m</span></td><td>&nbsp;</td><td><span>11h</span></td><td><span>1.412x10^18</span></td><td><span>2.2x10^9</span></td><td><span>2.5163</span></td><td><span>34.122</span></td><td><span>20.14</span></td><td><span>47.26</span></td><td><span>49.64</span></td><td><span>27.94</span></td><td><span>25.63</span></td></tr><tr><td><span>ParaLlama-αp-small</span></td><td><span>768</span></td><td><span>369m</span></td><td>&nbsp;</td><td><span>11h</span></td><td><span>1.412x10^18</span></td><td><span>2.2x10^9</span></td><td><span>2.5385</span></td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr><tr><td><span>ParaLlama-small-control (no embeddings)</span></td><td><span>768</span></td><td><span>119m</span></td><td><span>95m</span></td><td><span>6h</span></td><td><span>1.246x10^18</span></td><td><span>2.2x10^9</span></td><td><span>2.535</span></td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr><tr><td><a href='https://hf.co/crumb/Llama-p-small'><span>Llama-p-small</span></a><span> (w/ embeddings)</span></td><td><span>768</span></td><td><span>369m</span></td><td><span>101m</span></td><td><span>11h</span></td><td><span>1.412x10^18</span></td><td><span>2.2x10^9</span></td><td><span>2.485</span></td><td><span>34.658</span></td><td><span>21.67</span></td><td><span>46.89</span></td><td><span>51.22</span></td><td><span>28.27</span></td><td><span>25.24</span></td></tr></tbody></table></figure><h4 id='performance'><span>Performance</span></h4><p><span>I calculated a quick-and-dirty mean strided perplexity from the first 1k samples of the deduplicated pile with 512 token strides for other models trained on the Pile dataset as well as some control and our main experimental ParaLlama-p models.</span></p><figure class='table-figure'><table><thead><tr><th><span>Model / Arch / Nickname</span></th><th><span>Strided Sliding-Window Deduped Pile Perplexity</span></th><th><span>Avg </span></th><th><span>Hidden Size</span></th><th><span>Layers</span></th><th><span>Tokens (billions)</span></th></tr></thead><tbody><tr><td><span>pythia-70m-deduped</span></td><td><span>22.393400192260742</span></td><td><span>28.93</span></td><td><span>512</span></td><td><span>6</span></td><td><span>300b</span></td></tr><tr><td><span>pythia-160m-deduped</span></td><td><span>13.933751106262207</span></td><td><span>29.38</span></td><td><span>768</span></td><td><span>12</span></td><td><span>300b</span></td></tr><tr><td><span>pythia-410m-deduped</span></td><td><span>9.61842155456543</span></td><td><span>31.29</span></td><td><span>1024</span></td><td><span>24</span></td><td><span>300b</span></td></tr><tr><td><span>pythia-1b-deduped</span></td><td><span>7.889087677001953</span></td><td><span>32.78</span></td><td><span>2048</span></td><td><span>16</span></td><td><span>300b</span></td></tr><tr><td><span>pythia-1.4b-deduped</span></td><td><span>7.286991596221924</span></td><td><span>35</span></td><td><span>2048</span></td><td><span>24</span></td><td><span>300b</span></td></tr><tr><td><span>pythia-2.8b-deduped in 8-bit (sorry)</span></td><td>&nbsp;</td><td><span>36.72</span></td><td><span>2560</span></td><td><span>32</span></td><td><span>300b</span></td></tr><tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr><tr><td><span>Cerebras-GPT-111M</span></td><td><span>21.550655364990234</span></td><td><span>27.75</span></td><td><span>768</span></td><td><span>10</span></td><td><span>2.2b</span></td></tr><tr><td><span>Cerebras-GPT-256M</span></td><td><span>15.203496932983398</span></td><td><span>29.38</span></td><td><span>1088</span></td><td><span>14</span></td><td><span>5.1b</span></td></tr><tr><td><span>Cerebras-GPT-590M</span></td><td><span>12.098200798034668</span></td><td><span>29.14</span></td><td><span>1536</span></td><td><span>18</span></td><td><span>11.8B</span></td></tr><tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr><tr><td><span>ParaLlama-small-control</span></td><td><span>14.519485473632812</span></td><td>&nbsp;</td><td><span>768</span></td><td><span>10</span></td><td><span>2.2b</span></td></tr><tr><td><span>ParaLlama-αp-small</span></td><td><span>14.520842552185059</span></td><td>&nbsp;</td><td><span>768</span></td><td><span>10</span></td><td><span>2.2b (transformer) + 1400b* (embeddings)</span></td></tr><tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr><tr><td><a href='https://hf.co/crumb/ParaLlama-p-small'><span>ParaLlama-p-small</span></a></td><td><span>14.094217300415039</span></td><td><span>34.122</span></td><td><span>768</span></td><td><span>10</span></td><td><span>2.2b (transformer) + 1400b* (embeddings)</span></td></tr><tr><td><a href='https://hf.co/crumb/ParaLlama-p-medium'><span>ParaLlama-p-medium</span></a></td><td><span>10.680788040161133</span></td><td><span>35.018</span></td><td><span>1088</span></td><td><span>14</span></td><td><span>5.1b (transformer) + 1400b* (embeddings)</span></td></tr><tr><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr><tr><td><span>Llama-like (no embeds, llama-like composition of ffn &amp; attn)</span></td><td><span>13.882301330566406</span></td><td><span>34.33</span></td><td><span>768</span></td><td><span>10</span></td><td><span>2.2b</span></td></tr><tr><td><a href='https://hf.co/crumb/Llama-p-small'><span>Llama-p-small</span></a><span> (w/ embeds)</span></td><td><span>13.565109252929688</span></td><td><span>34.658</span></td><td><span>768</span></td><td><span>10</span></td><td><span>2.2b + 1400b* (embeddings)</span></td></tr></tbody></table></figure><p><em><span>*</span><span> not directly from pile but likely large overlap</span></em></p><p><span>I think that I got a really particularly lucky seed when testing if the parallel blocks work better. It&#39;s the 30th by the time I&#39;m training the new control &quot;just to see if it really did work just in case,&quot; and &quot;to fill out the table better,&quot; which turned out to be the right decision, with a very disappointing result. I did have just enough time to train a Llama-like model with the expanded embeddings from Llama2-70b though! </span></p><figure class='table-figure'><table><thead><tr><th><span>Architecture/Nickname</span></th><th><span>Dim</span></th><th><span>Max Parameters</span></th><th><span>Non-embed Parameters</span></th><th><span>Approximate duration</span></th><th><span>FLOPs</span></th><th><span>Tokens</span></th><th><span>Test loss</span></th><th><span>Average</span></th><th><span>ARC@25</span></th><th><span>TruthfulQA@0</span></th><th><span>WinoGrande@5</span></th><th><span>HellaSwag@10</span></th><th><span>MMLU@5</span></th></tr></thead><tbody><tr><td><a href='https://hf.co/crumb/Llama-p-small'><span>Llama-p-small</span></a></td><td><span>768</span></td><td><span>369m</span></td><td><span>101m</span></td><td><span>11h</span></td><td><span>1.412x10^18</span></td><td><span>2.2x10^9</span></td><td><span>2.483</span></td><td><span>34.658</span></td><td><span>21.67</span></td><td><span>46.89</span></td><td><span>51.22</span></td><td><span>28.27</span></td><td><span>25.24</span></td></tr><tr><td><span>Llama-like (no embeds, llama-like composition of ffn &amp; attn)</span></td><td><span>768</span></td><td><span>119m</span></td><td><span>95m</span></td><td><span>7h</span></td><td><span>1.246x10^18</span></td><td><span>2.2x10^9</span></td><td><span>2.503</span></td><td><span>34.33</span></td><td><span>20.65</span></td><td><span>46.14</span></td><td><span>50.28</span></td><td><span>27.72</span></td><td><span>26.86</span></td></tr></tbody></table></figure><p>&nbsp;</p><h2 id='some-more-things-that-didnt-work-very-well'><span>Some (More) Things that Didn&#39;t Work Very Well</span></h2><h4 id='virtual-token-models'><span>Virtual Token Models</span></h4><p><span>I also had an idea to split each token embedding into multiple &quot;virtual token embeddings&quot; to run side-by-side (eight 16d tokens could become sixteen 8d tokens), to increase the amount of information we&#39;re working with per natural language token while keeping the same size transformer, but we cant do that without a complete transformation of the token embeddings (any residual transformation here gives really bad loss), so I first add a </span><code>nn.Linear(max_dimension, max_dimension)</code><span> operation, then a custom </span><code>Split(virtual_tokens)</code><span> class after the </span><code>WTE</code><span> and a similar linear and &#39;recombine&#39; operation before the </span><code>LM_HEAD</code><span>. I have to test small here because training a VTM&#39;s transformer on </span><code>d</code><span>  natural language tokens is essentially training a &quot;normal&quot; transformer on </span><code>d*virtual_tokens</code><span> tokens, so I decrease the number of virtual tokens by making the first linear transformation go from </span><code>max_dimension</code><span> to a much smaller dimension that can still be split into multiple tokens. </span></p><p><span>Each virtual token uses the same </span><code>position_id</code><span> as it&#39;s origin natural language token.</span></p><figure class='table-figure'><table><thead><tr><th><span>Architecture/Nickname</span></th><th><span>Dim</span></th><th><span>Maximum Parameters</span></th><th><span>Approximate duration</span></th><th><span>FLOPs</span></th><th><span>Tokens</span></th><th><span>Test loss</span></th></tr></thead><tbody><tr><td><a href='https://hf.co/crumb/ParaLlama-p-micro'><span>ParaLlama-p-micro</span></a></td><td><span>256</span></td><td><span>273m</span></td><td><span>5.8h</span></td><td><span>1.384x10^17</span></td><td><span>2.2x10^9</span></td><td><span>3.11</span></td></tr><tr><td><span>2VTM-micro</span></td><td><span>.</span></td><td><span>344m</span></td><td><span>8h</span></td><td><span>1.083x10^18</span></td><td><span>2.2x10^9</span></td><td><span>3.018</span></td></tr><tr><td><span>3VTM-micro</span></td><td><span>.</span></td><td><span>349m</span></td><td><span>10h</span></td><td><span>1.143x10^18</span></td><td><span>2.2x10^9</span></td><td><span>2.996</span></td></tr><tr><td><span>4VTM-micro</span></td><td><span>.</span></td><td><span>353m</span></td><td><span>11h</span></td><td><span>1.204x10^18</span></td><td><span>2.2x10^9</span></td><td><span>2.994</span></td></tr></tbody></table></figure><p><span>The VTM models end up slightly better than a model that utilizes the same dimension but in comparison to models that use a similar number of flops per forward they&#39;re worse, if parameter count is to be prioritized above all else, this is a viable architecture.</span></p><h4 id='mixture-of-embeddings'><span>Mixture of Embeddings</span></h4><p><span>A mixture of embeddings model is the ParaLlama architecture, with a new Embedding class. This new Embedding class consists of a traditional embedding layer that turns a token into a vector the size of which we&#39;ll call the proxy embedding dimension, then one projection for each imported embedding is done on the proxy embeddings, from the proxy embedding dimension to the vocab size of each of the imported embeddings. We also, from the proxy embedding, with a linear layer, estimate an alpha for each resulting embedding out, after they&#39;re projected to the model&#39;s dimensions with another linear layer. We then just perform a weighted sum based on that alpha! The language modelling head is unchanged (a simple Linear layer without any warm start).</span></p><p><span>I only had enough time for one test of this architecture so the embeddings I used were from Llama2-70b, Pythia-12b, and UL2-20b. That&#39;s vocabs of size 32000 with 8192 dimensions, 50688 with 5120 dimensions, and 32128 with 4096 dimensions. A proxy embedding size of 8 was used with a hidden size of 768, 10 layers, and 12 attention heads.</span></p><figure class='table-figure'><table><thead><tr><th><span>Architecture/Nickname</span></th><th><span>Dim</span></th><th><span>Max Parameters</span></th><th><span>Non-embed Parameters</span></th><th><span>Approximate duration</span></th><th><span>FLOPs</span></th><th><span>Tokens</span></th><th><span>Test loss</span></th><th><span>Average</span></th><th><span>ARC@25</span></th><th><span>TruthfulQA@0</span></th><th><span>WinoGrande@5</span></th><th><span>HellaSwag@10</span></th><th><span>MMLU@5</span></th></tr></thead><tbody><tr><td><a href='https://hf.co/crumb/ParaLlama-p-small'><span>ParaLlama-p-small</span></a></td><td><span>768</span></td><td><span>369m</span></td><td>&nbsp;</td><td><span>11h</span></td><td><span>1.412x10^18</span></td><td><span>2.2x10^9</span></td><td><span>2.5163</span></td><td><span>34.122</span></td><td><span>20.14</span></td><td><span>47.26</span></td><td><span>49.64</span></td><td><span>27.94</span></td><td><span>25.63</span></td></tr><tr><td><span>MoEmbed-base</span></td><td><span>768</span></td><td><span>787m</span></td><td><span>119m</span></td><td><span>16.5h</span></td><td><span>1.760x10^18</span></td><td><span>2.2x10^9</span></td><td><span>2.754</span></td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td><td>&nbsp;</td></tr></tbody></table></figure><p><span>This is without using a mixture of language modelling heads but due to the time constraint set, I don&#39;t have enough time to implement and train a model with a mixture of language modelling heads.</span></p><h2 id='fin'><span>Fin</span></h2><p><span>if you want better benchmark score using the embeddings of a model that gets a better benchmark score is... something you can do that won&#39;t do a lot. I suspect that these models, since they have access to a very large embedding-space and only are utilizing a very small part of it, could be more effective when finetuned than base &quot;normal&quot; models finetuned but didn&#39;t have time to check.</span></p><h4 id='final-extra-notes'><span>Final (Extra) Notes</span></h4><ul><li><p><span>I didn&#39;t touch on flattening the models enough! Most of these models can have their embeddings &quot;flattened&quot; or &quot;collapsed&quot; since they&#39;re made of just a bunch of linear operations, you can combine them all onto the embedding layer itself and it should be the same size as a normal model&#39;s embedding layer! I did play around with it a very small amount and got some what I think are floating point calculation errors where the performance is slightly worse when running the flattened model on evals. In theory, though, you could find a way to merge the embedding layers operations into one reasonably small layer without loss degradation if you wanted to serve the base model cheaper. For finetuning though, optimizing the full operations should give a far better loss than finetuning the flattened version.</span></p></li></ul><ul><li><p><span>The approximate duration may not be consistent throughout the experiments, even little things like opening my window to allow a cold breeze speeds up the Vector, so it&#39;s mainly there as a &quot;guess&quot; to how long the model would take to train on a machine with 2 NVIDIA RTX-A6000 graphics cards and a Ryzen threadripper pro 5955wx 16-cores x 32. </span></p></li><li><p><span>I used Cerebras models as reference for how much data to train the models with, having a minimum amount of 2.2GT, but that most likely is not &quot;compute optimal,&quot; new scaling laws to fit training with specific pretrained embeddings would have to be estimated.</span></p></li><li><p><span>I launched 93 training runs for this by hand! If you want this kind of dedication to a project I&#39;m available for hire but don&#39;t hire me yet because I have an appointment I need my insurance for first but think about it jot my name down in a note come back to it in march </span><code>^-^</code><span> if you think I&#39;d fit in your team or if you want to give me money  for fun   which you should do </span><code>^-^</code></p></li></ul><ul><li><p><code>Llama-p-large-mixtral</code><span> aka is cooking while I come up with February&#39;s idea, it&#39;s 686m non-embedding parameters, 823m total, 1536 dimensions. Hoping to get 11.8GTok in</span></p></li></ul></div></div>

<script>(function(){function e(e,n,i){document.addEventListener(e,function(e){if(!e.defaultPrevented)for(var t=e.target;t&&t!=this;t=t.parentNode)if(t.matches(n)){!1===i.call(t,e)&&(e.preventDefault(),e.stopPropagation());break}},!1)}var t=document.body.parentElement,i=[],r=null,o=document.body.classList.contains("typora-export-collapse-outline");function a(){return t.scrollTop}e("click",".outline-expander",function(e){var t=this.closest(".outline-item-wrapper").classList;return t.contains("outline-item-open")?t.remove("outline-item-open"):t.add("outline-item-open"),u(),!1}),e("click",".outline-item",function(e){var t=this.querySelector(".outline-label");location.hash="#"+t.getAttribute("href"),o&&((t=this.closest(".outline-item-wrapper").classList).contains("outline-item-open")||t.add("outline-item-open"),d(),t.add("outline-item-active"))});function s(){var e=a();r=null;for(var t=0;t<i.length&&i[t][1]-e<60;t++)r=i[t]}function n(){c=setTimeout(function(){var n;i=[],n=a(),document.querySelector("#write").querySelectorAll("h1, h2, h3, h4, h5, h6").forEach(e=>{var t=e.getAttribute("id");i.push([t,n+e.getBoundingClientRect().y])}),s(),u()},300)}var l,c,d=function(){document.querySelectorAll(".outline-item-active").forEach(e=>e.classList.remove("outline-item-active")),document.querySelectorAll(".outline-item-single.outline-item-open").forEach(e=>e.classList.remove("outline-item-open"))},u=function(){if(r&&(d(),t=document.querySelector('.outline-label[href="#'+(CSS.escape?CSS.escape(r[0]):r[0])+'"]')))if(o){var e=t.closest(".outline-item-open>ul>.outline-item-wrapper");if(e)e.classList.add("outline-item-active");else{for(var t,n=(t=t.closest(".outline-item-wrapper")).parentElement.closest(".outline-item-wrapper");n;)n=(t=n).parentElement.closest(".outline-item-wrapper");t.classList.add("outline-item-active")}}else t.closest(".outline-item-wrapper").classList.add("outline-item-active")};window.addEventListener("scroll",function(e){l&&clearTimeout(l),l=setTimeout(function(){s(),u()},300)});window.addEventListener("resize",function(e){c&&clearTimeout(c),n()}),n()})();</script>

	  <script src="scripts/p5.min.js"></script>
  <!-- <script src="sketch32.js"></script> -->
  <script>
var scripts = ["scripts/sketch12.js", "scripts/sketch13.js", "scripts/sketch18.js", "scripts/sketch22.js", "scripts/sketch23.js", "scripts/sketch26.js", "scripts/sketch28.js", "scripts/sketch29.js", "scripts/sketch30.js", "scripts/sketch31.js"];

// Generate a random index between 0 and the length of the list
var index = Math.floor(Math.random() * scripts.length);

// Get the script name from the list
var scriptName = scripts[index];

// Create a script element with the src attribute set to the script name
var script = document.createElement("script");
script.src = scriptName;

// Append the script element to the body of the document
document.body.appendChild(script);
  </script>
</body>
</html>
